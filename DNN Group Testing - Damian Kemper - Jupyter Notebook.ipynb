import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import pulp
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, auc
from torch.optim.lr_scheduler import CosineAnnealingLR
import pandas as pd
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
# The implementation, including this Jupyter Notebook, Python code, and libraries runs inside the d2l environment.
# The d2l environment can be downloaded and installed at https://www.d2l.ai/chapter_installation/index.html

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Data generation and test design functions.
def generate_synthetic_data(n, t, k):
    L = math.ceil((math.log(2) * t) / k)
    adjacencymatrix = torch.zeros((t, n), dtype=torch.int32)

    for individual in range(n):
        chosen_tests = np.random.choice(t, L, replace=True)
        adjacencymatrix[chosen_tests, individual] = 1

    return adjacencymatrix

def choose_k_indices(n, k):
    result = torch.zeros(n, dtype=torch.int64)
    indices = torch.randperm(n)[:k]
    result[indices] = 1
    return result

def simulate_test_outcomes(M, defectives):
    testoutcomes = (np.dot(M.numpy(), defectives.numpy()) > 0).astype(int)
    return torch.tensor(testoutcomes, dtype=torch.int64)

def matrixtransforming(r, n, k, t):
    j = n * t + t
    final_matrix = torch.empty((r, j), dtype=torch.float32)
    final_goal_matrix = torch.empty((r, n), dtype=torch.float32)
    for i in range(r):
        test_matrix = generate_synthetic_data(n, t, k)
        defectives = choose_k_indices(n, k)
        test_results = simulate_test_outcomes(test_matrix, defectives)

        flattened_matrix = test_matrix.flatten()
        flattened_features = torch.cat((test_results, flattened_matrix), dim=0)

        final_matrix[i] = flattened_features
        final_goal_matrix[i] = defectives.float()

    return final_matrix, final_goal_matrix

# Data augmentation function.
def augment_data(x, y, augmentation_factor=3, b=0):
    # Finds samples with defective items (in our current implementation, all samples have k=1 defectives).
    # For a future implementation, with a randomly generated k, this function might be highly useful, if improved.
    # It would be able to help the DNN to focus its learning on harder samples (generally those with small k values).
    defective_mask = y.sum(dim=1) < b
    defective_samples = x[defective_mask]
    defective_labels = y[defective_mask]
    
    augmented_samples = []
    augmented_labels = []
    
    for _ in range(augmentation_factor):
        # Adds small noise to defective samples.
        noise = torch.randn_like(defective_samples) * 0.01
        augmented_samples.append(defective_samples + noise)
        augmented_labels.append(defective_labels)
    
    if len(augmented_samples) > 0:
        x_augmented = torch.cat([x] + augmented_samples, dim=0)
        y_augmented = torch.cat([y] + augmented_labels, dim=0)
        return x_augmented, y_augmented
    
    return x, y

# Inference algorithms.
def sss_ilp(X, y):
    num_tests, num_items = X.shape
    prob = pulp.LpProblem("Smallest_Satisfying_Set", pulp.LpMinimize)
    z = [pulp.LpVariable(f'z_{i}', cat=pulp.LpBinary) for i in range(num_items)]
    prob += pulp.lpSum(z)
    for t in range(num_tests):
        if y[t] == 0:
            prob += (pulp.lpDot(X[t, :], z) == 0)
        else:
            prob += (pulp.lpDot(X[t, :], z) >= 1)
    prob.solve(pulp.PULP_CBC_CMD(msg=0))
    if pulp.LpStatus[prob.status] != 'Optimal':
        return np.zeros(num_items, dtype=int)

    z_solution = np.array([int(pulp.value(var)) for var in z])
    return z_solution

def COMP(X, y):
    n = X.shape[1]
    result = np.ones(n, dtype=int)
    for g in range(X.shape[0]):
        specialcase, specialindividual = 0, 0
        for h in range(X.shape[1]):
            if X[g][h] == 1:
                if y[g] == 0:
                    result[h] = 0
                else:
                    specialcase += 1
                    specialindividual = h
        if y[g] == 1 and specialcase == 1:
            result[specialindividual] = 1
    return result

def dd(X, y):
    n, t = X.shape[1], X.shape[0]
    definitely_not = (X[y == 0].sum(axis=0) > 0)
    PD_mask = ~definitely_not
    DD_mask = np.zeros(n, dtype=bool)
    for test_idx in range(t):
        if y[test_idx] == 1:
            pd_in_test = np.where((X[test_idx] > 0) & PD_mask)[0]
            if pd_in_test.size == 1:
                DD_mask[pd_in_test[0]] = True
    return DD_mask.astype(int)


# Deep Neural Network Architecture.
class GroupTestingDNN(nn.Module):
    def __init__(self, input_size, output_size, hidden_sizes=[1024, 512, 256, 128]):
        super(GroupTestingDNN, self).__init__()
        
        layers = []
        prev_size = input_size
        
        # Builds hidden layers with regularization.
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.BatchNorm1d(hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.3))
            prev_size = hidden_size
        
        # Output layer with sigmoid for binary classification.
        layers.append(nn.Linear(prev_size, output_size))
        layers.append(nn.Sigmoid())
        
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)

# Loss Function.
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# Training function.
def train_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        
        # Gradient clipping for stability.
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        total_loss += loss.item()
    
    return total_loss / len(train_loader)

def validate_epoch(model, val_loader, criterion, device):
    model.eval()
    total_loss = 0
    all_predictions = []
    all_predictions_probs = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            total_loss += loss.item()
            
            all_predictions.append((output > 0.5).float().cpu())
            all_predictions_probs.append(output.cpu())
            all_targets.append(target.cpu())
    
    predictions = torch.cat(all_predictions, dim=0)
    predictions_probs = torch.cat(all_predictions_probs, dim=0)
    targets = torch.cat(all_targets, dim=0)
    
    # Calculates comprehensive metrics for the DNN (which has probabilities as outputs).
    metrics = calculate_comprehensive_metrics(
        targets, predictions, predictions_probs, output_type='probability'
    )
    
    return total_loss / len(val_loader), metrics, predictions, predictions_probs, targets

# Trains a DNN model for a specific t value.
def train_dnn_for_specific_t(x_train, y_train, x_val, y_val, x_test, y_test, input_size, output_size, t_value):
    print(f"Training DNN for t={t_value}, input_size={input_size}, output_size={output_size}")
    
    # Data loaders.
    train_dataset = torch.utils.data.TensorDataset(x_train, y_train)
    val_dataset = torch.utils.data.TensorDataset(x_val, y_val)
    test_dataset = torch.utils.data.TensorDataset(x_test, y_test)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # Initializes a DNN.
    model = GroupTestingDNN(input_size=input_size, output_size=output_size).to(device)
    
    # Calculates class weights for the data.
    # The results provide insights into how imbalanced the data is and prints them out. 
    pos_count = y_train.sum().item()
    neg_count = (y_train.numel() - pos_count)
    pos_weight = torch.tensor([neg_count / pos_count]).to(device)
    
    # Chooses the loss function.
    criterion = FocalLoss(alpha=2, gamma=2)
    
    # Optimizer with weight decay.
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    
    # Learning rate scheduler.
    scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)
    
    # Training loop with tracking.
    num_epochs = 50
    best_f1 = 0
    best_model_state = None
    
    # Lists to track training progress.
    train_losses = []
    val_losses = []
    val_f1_scores = []
    overfitting_monitor = []
    
    for epoch in range(num_epochs):
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_metrics, _, _, _ = validate_epoch(model, val_loader, criterion, device)
        
        scheduler.step()
        
        # Stores training progress.
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_f1_scores.append(val_metrics['macro_f1'])
        overfitting_monitor.append(val_loss - train_loss)  # Positive values indicate overfitting
        
        # Saves best model based on macro-F1 score.
        if val_metrics['macro_f1'] > best_f1:
            best_f1 = val_metrics['macro_f1']
            best_model_state = model.state_dict().copy()
        
        if epoch % 10 == 0:
            print(f"  Epoch {epoch:2d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, "
                  f"Val F1={val_metrics['macro_f1']:.4f}")
    
    # Loads best model.
    model.load_state_dict(best_model_state)
    
    # Evaluation on test set.
    final_loss, final_metrics, test_predictions, test_predictions_probs, test_targets = validate_epoch(model, test_loader, criterion, device)
    
    create_training_plots(train_losses, val_losses, overfitting_monitor, t_value)
    create_test_evaluation_plots(test_targets, test_predictions, final_metrics['f1_scores_per_label'], t_value)
    
    return final_metrics


# Performance metrics calculation.
def calculate_comprehensive_metrics(y_true, y_pred, y_pred_probs=None, output_type='probability', threshold=0.5):
    # Converts the predictions of the DNN to binary values.
    if output_type == 'probability' and y_pred_probs is not None:
        y_pred_binary = (y_pred_probs > threshold).float()
    else:
        y_pred_binary = y_pred.float()
    
    # Converts to numpy for easier computation.
    y_true_np = y_true.numpy()
    y_pred_np = y_pred_binary.numpy()
    
    # 1. Hamming Accuracy (1 - Hamming Loss).
    hamming_accuracy = 1 - np.mean(y_true_np != y_pred_np)
    
    # 2. Exact Match Ratio (Subset Accuracy).
    exact_match_ratio = np.mean(np.all(y_true_np == y_pred_np, axis=1))
    
    # 3. Jaccard Index (Intersection over Union).
    intersection = np.logical_and(y_true_np, y_pred_np).sum(axis=1)
    union = np.logical_or(y_true_np, y_pred_np).sum(axis=1)
    jaccard_index = np.mean(intersection / (union + 1e-8))
    
    # 4. Precision (sample-wise).
    tp = np.logical_and(y_true_np, y_pred_np).sum(axis=1)
    fp = np.logical_and(np.logical_not(y_true_np), y_pred_np).sum(axis=1)
    precision = np.mean(tp / (tp + fp + 1e-8))
    
    # 5. Sensitivity/Recall (sample-wise).
    fn = np.logical_and(y_true_np, np.logical_not(y_pred_np)).sum(axis=1)
    sensitivity = np.mean(tp / (tp + fn + 1e-8))
    
    # 6. Macro-F1 Score (average F1 across all labels).
    f1_scores = []
    for i in range(y_true_np.shape[1]):
        tp_i = np.sum((y_true_np[:, i] == 1) & (y_pred_np[:, i] == 1))
        fp_i = np.sum((y_true_np[:, i] == 0) & (y_pred_np[:, i] == 1))
        fn_i = np.sum((y_true_np[:, i] == 1) & (y_pred_np[:, i] == 0))
        
        prec_i = tp_i / (tp_i + fp_i + 1e-8)
        rec_i = tp_i / (tp_i + fn_i + 1e-8)
        f1_i = 2 * prec_i * rec_i / (prec_i + rec_i + 1e-8)
        f1_scores.append(f1_i)
    
    macro_f1 = np.mean(f1_scores)
    
    # 7. Specificity (True Negative Rate).
    tn = np.logical_and(np.logical_not(y_true_np), np.logical_not(y_pred_np)).sum(axis=1)
    specificity = np.mean(tn / (tn + fp + 1e-8))
    
    return {
        'hamming_accuracy': hamming_accuracy,
        'exact_match_ratio': exact_match_ratio,
        'jaccard_index': jaccard_index,
        'precision': precision,
        'sensitivity': sensitivity,
        'macro_f1': macro_f1,
        'specificity': specificity,
        'f1_scores_per_label': f1_scores
    }



def evaluate_classical_algorithms_for_t(x_test_clean, y_test_clean, t, individuals):
    # Results storage.
    dd_results = []
    sss_ilp_results = []
    comp_results = []
    
    
    for i in tqdm(range(x_test_clean.shape[0]), desc="  Processing samples"):
        # Extracts test matrix and outcomes.
        test_outcomes = x_test_clean[i, :t].numpy().astype(int)
        test_matrix = x_test_clean[i, t:].reshape(t, individuals).numpy()
        
        # DD Algorithm.
        try:
            dd_pred = dd(test_matrix, test_outcomes)
            dd_results.append(dd_pred)
        except Exception as e:
            dd_results.append(np.zeros(individuals, dtype=int))
        
        # SSS Algorithm.
        try:
            sss_pred = sss_ilp(test_matrix, test_outcomes)
            sss_ilp_results.append(sss_pred)
        except Exception as e:
            sss_ilp_results.append(np.zeros(individuals, dtype=int))
        
        # COMP Algorithm.
        try:
            comp_pred = COMP(test_matrix, test_outcomes)
            comp_results.append(comp_pred)
        except Exception as e:
            comp_results.append(np.zeros(individuals, dtype=int))
    
    # Calculates metrics for each algorithm.
    dd_preds = torch.tensor(np.array(dd_results), dtype=torch.float32)
    sss_ilp_preds = torch.tensor(np.array(sss_ilp_results), dtype=torch.float32)
    comp_preds = torch.tensor(np.array(comp_results), dtype=torch.float32)
    
    dd_metrics = calculate_comprehensive_metrics(
        y_test_clean, dd_preds, output_type='binary'
    )
    sss_ilp_metrics = calculate_comprehensive_metrics(
        y_test_clean, sss_ilp_preds, output_type='binary'
    )
    comp_metrics = calculate_comprehensive_metrics(
        y_test_clean, comp_preds, output_type='binary'
    )
    
    return {
        'DD': dd_metrics,
        'SSS_ILP': sss_ilp_metrics,
        'COMP': comp_metrics
    }

# Visualization.
# Creates training and validation loss plots, and the overfitting monitor.
def create_training_plots(train_losses, val_losses, overfitting_monitor, t_value):
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    epochs = range(1, len(train_losses) + 1)
    
    # Plot 1: Training and Validation Loss.
    axes[0].plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)
    axes[0].plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title(f'Training and Validation Loss (t={t_value})')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Plot 2: Overfitting Monitor.
    axes[1].plot(epochs, overfitting_monitor, 'g-', linewidth=2)
    axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
    axes[1].fill_between(epochs, overfitting_monitor, 0, 
                        where=[x > 0 for x in overfitting_monitor], 
                        color='red', alpha=0.3)
    axes[1].fill_between(epochs, overfitting_monitor, 0, 
                        where=[x <= 0 for x in overfitting_monitor], 
                        color='green', alpha=0.3)
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Val Loss - Train Loss')
    axes[1].set_title(f'Overfitting Monitor (t={t_value})')
    axes[1].grid(True, alpha=0.3)
    
    # Plot 3: Loss Difference Trend.
    axes[2].plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2, alpha=0.7)
    axes[2].plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2, alpha=0.7)
    axes[2].fill_between(epochs, train_losses, val_losses, alpha=0.2, color='orange')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Loss')
    axes[2].set_title(f'Loss Gap Visualization (t={t_value})')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# Creates confusion matrix and F1-score distribution plots.
def create_test_evaluation_plots(test_targets, test_predictions, f1_scores_per_label, t_value):
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Converts to numpy for easier processing.
    test_targets_np = test_targets.numpy()
    test_predictions_np = test_predictions.numpy()
    
    # Plot 1: Confusion Matrix (summed over all labels).
    tp_total = np.sum((test_targets_np == 1) & (test_predictions_np == 1))
    fp_total = np.sum((test_targets_np == 0) & (test_predictions_np == 1))
    tn_total = np.sum((test_targets_np == 0) & (test_predictions_np == 0))
    fn_total = np.sum((test_targets_np == 1) & (test_predictions_np == 0))
    
    confusion_matrix_aggregate = np.array([[tn_total, fp_total],
                                         [fn_total, tp_total]])
    
    # Creates heatmap for confusion matrix.
    sns.heatmap(confusion_matrix_aggregate, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Predicted Negative', 'Predicted Positive'],
                yticklabels=['Actual Negative', 'Actual Positive'],
                ax=axes[0])
    axes[0].set_title(f'Aggregated Confusion Matrix (t={t_value})')
    axes[0].set_xlabel('Predicted')
    axes[0].set_ylabel('Actual')
    
    # Plot 2: Box Plot of F1-score Distribution Across Labels.
    axes[1].boxplot(f1_scores_per_label, patch_artist=True,
                   boxprops=dict(facecolor='lightblue', alpha=0.7),
                   medianprops=dict(color='red', linewidth=2))
    axes[1].set_ylabel('F1-Score')
    axes[1].set_xlabel('Distribution Across All Labels')
    axes[1].set_title(f'F1-Score Distribution Across Labels (t={t_value})')
    axes[1].grid(True, alpha=0.3)
    
    # Adds some statistics as text into the box plot, like mean, standard deviation, and min and max macro-F1 scores.
    mean_f1 = np.mean(f1_scores_per_label)
    std_f1 = np.std(f1_scores_per_label)
    min_f1 = np.min(f1_scores_per_label)
    max_f1 = np.max(f1_scores_per_label)
    
    stats_text = f'Mean: {mean_f1:.3f}\nStd: {std_f1:.3f}\nMin: {min_f1:.3f}\nMax: {max_f1:.3f}'
    axes[1].text(0.7, 0.95, stats_text, transform=axes[1].transAxes, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7),
                verticalalignment='top')
    
    plt.tight_layout()
    plt.show()

# Creates 7 plots showing the performance vs t for each metric.
def create_performance_vs_t_plots(results, t_values):
    metrics_to_plot = [
        ('hamming_accuracy', 'Hamming Accuracy'),
        ('exact_match_ratio', 'Exact Match Ratio'),
        ('jaccard_index', 'Jaccard Index'),
        ('macro_f1', 'Macro-F1 Score'),
        ('specificity', 'Specificity'),
        ('sensitivity', 'Sensitivity'),
        ('precision', 'Precision')
    ]
    
    colors = {'DNN': '#1f77b4', 'DD': '#ff7f0e', 'SSS_ILP': '#2ca02c', 'COMP': '#d62728'}
    markers = {'DNN': 'o', 'DD': 's', 'SSS_ILP': '^', 'COMP': 'D'}
    
    # Creates a single figure with 3x3 subplots.
    fig = plt.figure(figsize=(15, 12))
    
    for idx, (metric_key, metric_name) in enumerate(metrics_to_plot):
        ax = plt.subplot(3, 3, idx + 1)
        
        for method in ['DNN', 'DD', 'SSS_ILP', 'COMP']:
            ax.plot(t_values, results[method][metric_key], 
                    marker=markers[method], linewidth=2.5, markersize=8, 
                    color=colors[method], label=method, alpha=0.8)
        
        ax.set_xlabel('Number of Tests (t)', fontsize=12)
        ax.set_ylabel(metric_name, fontsize=12)
        ax.set_title(f'{metric_name}', fontsize=14)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=10, loc='best')
        ax.set_ylim(0, 1.05)
        
        for method in ['DNN', 'DD', 'SSS_ILP', 'COMP']:
            for i, (t_val, score) in enumerate(zip(t_values, results[method][metric_key])):
                ax.annotate(f'{score:.3f}', 
                           (t_val, score), 
                           textcoords="offset points", 
                           xytext=(0,10), 
                           ha='center', fontsize=8,
                           bbox=dict(boxstyle="round,pad=0.3", facecolor=colors[method], alpha=0.3))
    
    fig.suptitle('Performance vs Number of Tests (t)\n', 
                 fontsize=16, y=0.98)
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.92)
    plt.show()

# Main function to evaluate the DNN and all classical algorithms across different t values.
def evaluate_DNN_and_algorithms_across_t_values(individuals=5, k=1, r=100000, r_comparison=1000, 
                                       t_values=[2, 3, 4]):

    print(f"{'='*90}")
    print(f"EVALUATING ALGORITHMS ACROSS DIFFERENT NUMBER OF TESTS (t)")
    print(f"{'='*90}")
    print(f"Parameters:")
    print(f"  - Individuals (n): {individuals}")
    print(f"  - Defective items (k): {k}")
    print(f"  - Training samples (r): {r}")
    print(f"  - Classical test samples (r_comparison): {r_comparison}")
    print(f"  - Test values (t): {t_values}")
    print(f"  - Device: {device}")
    print(f"{'='*90}")
    
    # Storage for results.
    metrics_names = ['hamming_accuracy', 'exact_match_ratio', 'jaccard_index', 
                     'macro_f1', 'specificity', 'sensitivity', 'precision']
    
    results = {}
    for method in ['DNN', 'DD', 'SSS_ILP', 'COMP']:
        results[method] = {metric: [] for metric in metrics_names}
    
    # Evaluates for each t value.
    for t in t_values:
        print(f"\n{'='*80}")
        print(f"EVALUATING FOR t = {t}")
        print(f"{'='*80}")
        
        # Calculates input size for this t of the for loop.
        j = individuals * t + t
        
        # Generates new datasets for this t of the for loop.
        x, y = matrixtransforming(r, individuals, k, t)
        
        # The next line would apply the data augmentation, but it is turned inactive instead.
        x, y = augment_data(x, y, augmentation_factor=3, b=0)
        
        # Splits data.
        x_temp, x_test, y_temp, y_test = train_test_split(
            x, y, test_size=0.2, random_state=42, stratify=y.sum(dim=1)
        )
        
        x_train, x_val, y_train, y_val = train_test_split(
            x_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp.sum(dim=1)
        )
        
        print(f"Data split - Train: {x_train.shape[0]}, Val: {x_val.shape[0]}, Test: {x_test.shape[0]}")
        
        # Trains DNN for this specific t of the for loop.
        dnn_metrics = train_dnn_for_specific_t(x_train, y_train, x_val, y_val, x_test, y_test, j, individuals, t)
        
        # Generates clean test data (data that is by definition never augmented) for the classical algorithms.
        x_test_clean, y_test_clean = matrixtransforming(r_comparison, individuals, k, t)
        
        # Evaluates classical algorithms.
        classical_metrics = evaluate_classical_algorithms_for_t(x_test_clean, y_test_clean, t, individuals)
        
        # Stores results.
        for metric in metrics_names:
            results['DNN'][metric].append(dnn_metrics[metric])
            results['DD'][metric].append(classical_metrics['DD'][metric])
            results['SSS_ILP'][metric].append(classical_metrics['SSS_ILP'][metric])
            results['COMP'][metric].append(classical_metrics['COMP'][metric])
        
        # Prints results for this t.
        print(f"\nResults for t={t}:")
        print("-" * 80)
        for metric in metrics_names:
            print(f"{metric.replace('_', ' ').title():20s}: "
                  f"DNN={dnn_metrics[metric]:.4f}, "
                  f"DD={classical_metrics['DD'][metric]:.4f}, "
                  f"SSS_ILP={classical_metrics['SSS_ILP'][metric]:.4f}, "
                  f"COMP={classical_metrics['COMP'][metric]:.4f}")
    
    # Creates the performance vs t plots.
    print(f"\n{'='*80}")
    print("CREATING PERFORMANCE VS T PLOTS")
    print(f"{'='*80}")
    
    create_performance_vs_t_plots(results, t_values)
    
    print(f"\n{'='*80}")
    print("FINAL SUMMARY - PERFORMANCE ACROSS T VALUES")
    print(f"{'='*80}")
    
    # Creates a summary table.
    summary_df = pd.DataFrame()
    for method in ['DNN', 'DD', 'SSS_ILP', 'COMP']:
        for metric in metrics_names:
            values = results[method][metric]
            row_data = {f't={t}': f'{val:.4f}' for t, val in zip(t_values, values)}
            row_data['Method'] = method
            row_data['Metric'] = metric.replace('_', ' ').title()
            summary_df = pd.concat([summary_df, pd.DataFrame([row_data])], ignore_index=True)
    
    # Prints summary by metric.
    for metric in metrics_names:
        print(f"\n{metric.replace('_', ' ').title()}:")
        print("-" * 60)
        for method in ['DNN', 'DD', 'SSS_ILP', 'COMP']:
            values = results[method][metric]
            values_str = ' '.join([f'{val:.4f}' for val in values])
            print(f"{method:15s}: {values_str}")
    
    return results

# Main execution.
if __name__ == "__main__":
    # Here the parameters can be set.
    individuals = 5 # Number of total individuals n (n=individuals).
    k = 1 # Number of defectives.
    r = 100  # Data set size for the DNN.
    r_comparison = 1000  # The size of the clean test data set for the classical algorithms.
    t_values = [2, 3, 4] # Numbers of different test sizes t.
    
    print(f"Device: {device}")
    
    # Runs the evaluation across t values.
    results = evaluate_DNN_and_algorithms_across_t_values(
        individuals=individuals, 
        k=k, 
        r=r, 
        r_comparison=r_comparison, 
        t_values=t_values
    )
    
    print(f"\n{'='*80}")
    print("EVALUATION COMPLETED!")
    print(f"{'='*80}")
    
    # Saves the results
    import pickle
    with open('performance_vs_t_results.pkl', 'wb') as f:
        pickle.dump(results, f)
    

